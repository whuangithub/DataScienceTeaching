import numpy as np
# import a package called "numpy" from installed packages ~/anaconda3/pkgs
# as... in this .py file, the imported package will be called "np"
from numpy.random import random, randn
# In the numpy package, there is a part called "random" which contains some functions to generate random numbers from a uniform distribution
# from that part, we import two functions: random (generate random numbers) and randn (generate random numbers from standard normal distribution)
from matplotlib import pyplot as plt, animation, rcParams, rc
# matplotlib https://matplotlib.org
# rc rcParams: https://matplotlib.org/api/matplotlib_configuration_api.html#matplotlib.rc modify the default settings in matplotlib
# pyplot: https://matplotlib.org/api/_as_gen/matplotlib.pyplot.html?highlight=pyplot#module-matplotlib.pyplot plotting
# animation: https://matplotlib.org/api/animation_api.html?highlight=animation#module-matplotlib.animation We are going to use the "FuncAnimation"

# when you find the code imports some packages that you don't know, searching the name of the package usually helps.

def linear_function(alpha,beta,x): return alpha+beta*x
# define a function called "linear_function". Inputs are alpha, beta, and x; output is calculated based on these there inputs alpha+beta*x
rc('animation',html='html5') # setting one option for animation function
rcParams['figure.figsize'] = 3, 3 # setting the size of figures
# modify the default settings for matplotlib

# In the next part, the code generates the artificial data.
beta_true = 6.0 # setting the true parameter for beta. The sign "=" is used to assign new value to the variable "beta_true"
alpha_true = 3.0 # setting the true parameter for alpha
n = 30 # setting n: sample size
x = random(n)
# The function "random" is imported from the numpy.random
# "random(n)" will call the "random" function and use "n" as the input of it.
# Then the output of "random" function (a random vertor constains n=30 elements) is assigned to the variable "x"
y = linear_function(alpha_true,beta_true,x)+0.2*randn(n)
# similar to the previous line: some values are generated by the RHS and assigned to variable "y"
# On the RHS, the result is the sum of two numbers: linear_function(alpha_true,beta_true,x) and 0.2*randn(n)
# "linear_function" is a function we defined before, "alpha_true", "beta_true", and "x" are 3 inputs
# similar to "random", "randn" is imported from numpy. The output of "randn(n)" is multiplied by 0.2
# y is a vector which contains n=30 elements
# The artificial data is generated ("x" and "y")


beta = -1. # assign value to the initial guess of beta: "beta"
alpha = 3. # assign value to the initial guess of alpha: "alpha"
learning_rate = 0.01 # let me explain this "learning_rate"

def update_alpha_and_beta(): # defining a function called "update_alpha_and_beta"
		global alpha, beta
		# global means that alpha, beta in this function are global global variables
		# when the value of alpha and beta changes, not only in this function but also the alpha, beta out of this function will change
		y_predicted = linear_function(alpha,beta,x)
		derivative_of_loss_wrt_alpha = 2*(y_predicted-y)
		derivative_of_loss_wrt_beta = x* derivative_of_loss_wrt_alpha
		alpha = alpha - learning_rate*derivative_of_loss_wrt_alpha.mean() # "derivative_of_loss_wrt_alpha" is a vertor, ".mean()" calculates the mean of the vertor
		beta = beta - learning_rate*derivative_of_loss_wrt_beta.mean() # Can you derive the function form of the gradient by yourself?
		# There is no "return" in this function, since the value of new alpha and beta is saved in global variables.
		# Each time we call this function, the value of "alpha" and "beta" is automatically saved.
def animate(i):
		x = np.linspace(0,1,100)
		# "linspace": https://docs.scipy.org/doc/numpy/reference/generated/numpy.linspace.html return 100 evenly spaced numbers between 0 and 1
		y = linear_function(alpha,beta,x)
		line.set_data(x,y)
		# the variable "line" is the identical variable in the following code
		# it is not a global variable in this function
		# we are not assigning new values to the variable (line = ...), but changing the property of it.
		for i in range(20): # for each time we call the animate function, we update the value of alpha and beta 20 times
				update_alpha_and_beta()
		return (line,)
		# return a "tuple": a finite ordered list (sequence) of elements
		# tuple: parentheses with comma (x,a,xaa,xdsaa1)
		# tuple with one element (a,)
		# the elements in tuple cannot be modified; the elements in list can be modified

fig = plt.figure(dpi=80,figsize=(7,7)) # we reset the figure size here
ax = fig.add_subplot(111) # https://matplotlib.org/api/_as_gen/matplotlib.figure.Figure.html add subplot to the figure
ax.set_xlim((0,1)) # set the range of x: 0 to 1
ax.set_ylim((-2,15)) # set the range of y
plt.scatter(x,y) # add points to the figure
line, = ax.plot([],[],lw=2) # add one empty/Null line to the figure, and we will change "line" in the "animate" function
# "line,":  unpacking a single value into line
# a, b = [1, 2]
# a, = [1, ]

resulting_animation = animation.FuncAnimation(fig,animate,np.arange(0,250),interval=100)
# call "animate" 250 times, generate 250 frames

Writer = animation.writers['ffmpeg'] # use ffmpeg to write/save the animation
writer = Writer(fps=15, metadata=dict(artist='Me'), bitrate=1800) # 15 frames per second
resulting_animation.save('resulting_animation4.mp4', writer=writer) # save to "resulting_animation.mp4"

print('Done.') # print 'Done.' on the screen

print(alpha)
print(beta)

# restart the kernal after comment out the line in Jupyter notebook (send to piazza as well)
# for eahc time we call the animate function, the value of alpha and beta will be updated 20 times
# when you generate a animation in jupyter notebook, the initial value of alpha and beta are changed.
# you should restart the kernal after deleting the line.




# image resize (mathematica) try it on your computer
